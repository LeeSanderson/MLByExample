{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3672c19",
   "metadata": {},
   "source": [
    "# Q-Learning By Example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b71f8f",
   "metadata": {},
   "source": [
    "## A Simple Example\n",
    "\n",
    "This environment consists of 6 states, labeled S0 through S5, arranged linearly:\n",
    "\n",
    "```text\n",
    "S0 — S1 — S2 — S3 — S4 — S5\n",
    "```\n",
    "\n",
    "- S2 is the start State. Every episode starts in this state.\n",
    "- S0 and S5 are terminal (end) States. When either of these states is reached the episode ends.\n",
    "- Rewards:\n",
    "    - Reaching S0 gives a reward of -1\n",
    "    - Reaching S5 gives a reward of +2\n",
    "    - All other transitions yield 0 reward\n",
    "\n",
    "**Actions**\n",
    "\n",
    "In each non-terminal state (S1 to S4), the agent has two possible actions:\n",
    "\n",
    " - Action a1: Move left (to a lower-numbered state)\n",
    "   _Example_: From S2, action a1 moves the agent to S1\n",
    "\n",
    " - Action a2: Move right (to a higher-numbered state)\n",
    "    _Example_: From S2, action a2 moves the agent to S3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc5ade5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a27825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = [-1, 0, 0, 0, 0, 2]\n",
    "q_values = [[0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0],\n",
    "            [0.0, 0.0]]\n",
    "end_states = [True, False, False, False, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b07fa162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action(state_index: int, epsilon: float = 0.1) -> int:\n",
    "    random = np.random.uniform()\n",
    "    if random < epsilon:\n",
    "        return np.random.randint(0, 2) # Random action\n",
    "    else:\n",
    "        return np.argmax(q_values[state_index]) # Best action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a31ba523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(current_state_index: int, action: int) -> tuple[int, int]:\n",
    "    if action == 0:  # Move left\n",
    "        next_state_index = current_state_index - 1\n",
    "    else:  # Move right\n",
    "        next_state_index = current_state_index + 1\n",
    "    return rewards[next_state_index], next_state_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aefcd556",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPISODES = 1000\n",
    "GAMMA = 0.9\n",
    "INITIAL_STATE_INDEX = 2\n",
    "epsilon = 0.9\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state_index = INITIAL_STATE_INDEX\n",
    "\n",
    "    while not end_states[state_index]:\n",
    "        action = epsilon_greedy_action(state_index, epsilon)\n",
    "        reward, next_state_index = take_action(state_index, action)\n",
    "\n",
    "        if end_states[next_state_index]:\n",
    "            q_values[state_index][action] = reward\n",
    "        else:\n",
    "            q_values[state_index][action] = reward + GAMMA * np.max(q_values[next_state_index])\n",
    "\n",
    "        state_index = next_state_index\n",
    "\n",
    "    epsilon = epsilon - (1 / NUM_EPISODES)  # Decay epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a83065b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.0],\n",
       " [-1, np.float64(1.4580000000000002)],\n",
       " [np.float64(1.3122000000000003), np.float64(1.62)],\n",
       " [np.float64(1.4580000000000002), np.float64(1.8)],\n",
       " [np.float64(1.62), 2],\n",
       " [0.0, 0.0]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44bd667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking action 1 in state 2, received reward 0, moving to state 3\n",
      "Taking action 1 in state 3, received reward 0, moving to state 4\n",
      "Taking action 1 in state 4, received reward 2, moving to state 5\n"
     ]
    }
   ],
   "source": [
    "state_index = INITIAL_STATE_INDEX\n",
    "epsilon = 0\n",
    "while not end_states[state_index]:\n",
    "    action = epsilon_greedy_action(state_index, epsilon)\n",
    "    reward, next_state_index = take_action(state_index, action)\n",
    "    print(f\"Taking action {action} in state {state_index}, received reward {reward}, moving to state {next_state_index}\")\n",
    "    state_index = next_state_index\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
