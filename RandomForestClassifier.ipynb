{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0285ba1a",
   "metadata": {},
   "source": [
    "# Random Forest Classifiers By Example\n",
    "\n",
    "\n",
    "Random forests build on the simplicity of [decision trees](https://www.sixsideddice.com/Blog/MLByExample/DecisionTreeClassifiersByExample.html) to create a more powerful and robust classification algorithm. They are widely used for classification tasks where accuracy and generalization are important, such as predicting customer churn, detecting fraud, or classifying images.\n",
    "\n",
    "In this article, we will explore how [SciKit Learn's Random Forests](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) can improve classification by combining many decision trees to predict survivors of the Titanic. As part of this journey we will examine additional features of the Titanic dataset and their predictive power, we will look at feature importance, and we will end with a discussion on handling missing data (and whether it is needed for Random Forests).\n",
    "\n",
    "This is the second article in the series \"ML by Example\". In my previous article, [Decision Tree Classifiers By Example](https://www.sixsideddice.com/Blog/MLByExample/DecisionTreeClassifiersByExample.html), I covered topics like the Titanic dataset, one-hot encoding, and train/test splits in detail. To avoid repetition, I will skip most of those details here. Please refer to that article if you want a deeper explanation.\n",
    "\n",
    "## How Random Forests Work\n",
    "\n",
    "A random forest is essentially a \"forest\" of decision trees that work together.\n",
    "\n",
    "- Instead of relying on a single decision tree, it builds many trees during training.\n",
    "- Each tree is trained on a different random subset of the data and features.\n",
    "- When making a prediction, every tree in the forest votes for a class.\n",
    "- The forest chooses the class with the most votes as the final prediction.\n",
    "\n",
    "This approach helps overcome limitations of individual trees, such as overfitting or being overly sensitive to noise.\n",
    "\n",
    "By averaging the predictions of many diverse trees, random forests tend to be more accurate and robust, while still retaining the interpretability and structure of decision trees at the individual tree level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86d13ee",
   "metadata": {},
   "source": [
    "## Python Prerequisites\n",
    "\n",
    "Let's install and import the prerequisites so they are ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f19091e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --quiet --upgrade pip \n",
    "# %pip install numpy --quiet\n",
    "# %pip install PyArrow --quiet\n",
    "# %pip install Pandas --quiet\n",
    "# %pip install scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fee7a15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd611eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data = pd.read_csv(\"Data/titanic_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6542d",
   "metadata": {},
   "source": [
    "\n",
    "And let's redefine our one-hot encoding utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e090a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(df : pd.DataFrame, column_name: str) -> tuple[pd.DataFrame, list[str]]:\n",
    "    categories = [f\"{column_name}_{value}\" for value in df[column_name].unique()]\n",
    "\n",
    "    # remove the categorical variables (if we previous called onehot_encode)\n",
    "    df = df.drop(categories, axis=1, errors=\"ignore\") \n",
    "    temp_column_name = f\"{column_name}_Temp\"\n",
    "\n",
    "    # get_dummies will remove the original column, so copy the data to temp column\n",
    "    df[temp_column_name] = df[column_name] \n",
    "    df = pd.get_dummies(df, prefix=column_name, columns=[temp_column_name], dtype=float)\n",
    "    return df, categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65247bb5",
   "metadata": {},
   "source": [
    "\n",
    "## Predicting Survivors\n",
    "\n",
    "Let's define a utility function we can use to train and evaluate the `RandomForestClassifier`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70a27e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(data: pd.DataFrame, base_predictors: list[str]) -> None:\n",
    "    \"\"\"Trains the model and evaluates it on the validation data.\"\"\"\n",
    "    data, gender_categories = onehot_encode(data, \"Sex\")\n",
    "    data, class_categories = onehot_encode(data, \"Pclass\")\n",
    "    predictors = base_predictors + gender_categories + class_categories\n",
    "    prediction = \"Survived\"\n",
    "\n",
    "    train, validate = (\n",
    "        train_test_split(\n",
    "            data, \n",
    "            test_size=0.2, \n",
    "            stratify=data[prediction], \n",
    "            random_state=42)\n",
    "        )\n",
    "\n",
    "    x = train[predictors]\n",
    "    y = train[[prediction]].values\n",
    "\n",
    "    random_forest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    random_forest.fit(x, y.ravel())\n",
    "\n",
    "    print(f\"Model trained with predictors: {predictors}\")\n",
    "    print(f\"Feature importances:\")\n",
    "    for feature, importance in zip(predictors, random_forest.feature_importances_):\n",
    "        print(f\" - {feature}: {importance:.3f}\")\n",
    "\n",
    "    print(f\"Number of trees: {len(random_forest.estimators_)}\")    \n",
    "\n",
    "    predictions = random_forest.predict(validate[predictors])\n",
    "    actuals = validate[[prediction]].values\n",
    "\n",
    "    score = accuracy_score(actuals, predictions)\n",
    "    print(f'Model accuracy: {score *100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269b526",
   "metadata": {},
   "source": [
    "\n",
    "Following the same hypothesis as we did in [Decision Tree Classifiers By Example](https://www.sixsideddice.com/Blog/MLByExample/DecisionTreeClassifiersByExample.html), we start with a simple prediction using `Age`, `Sex`, and `Pclass`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4036c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained with predictors: ['Age', 'Sex_male', 'Sex_female', 'Pclass_3', 'Pclass_1', 'Pclass_2']\n",
      "Feature importances:\n",
      " - Age: 0.442\n",
      " - Sex_male: 0.224\n",
      " - Sex_female: 0.176\n",
      " - Pclass_3: 0.081\n",
      " - Pclass_1: 0.057\n",
      " - Pclass_2: 0.021\n",
      "Number of trees: 100\n",
      "Model accuracy: 81.56%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_and_evaluate_model(titanic_data, [\"Age\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e34be",
   "metadata": {},
   "source": [
    "\n",
    "Great. This is more accurate than our best Decision Tree classifier (which was 80.45%).\n",
    "\n",
    "The output from the evaluation process also include the **feature importance** for each of the predictor columns. As it's name suggests, it is a way to measure how important each feature is in making predictions across the entire forest. The higher the number, the more that feature contributes to the model's decisions. The values are normalized to sum to 1, so you can interpret them as fractions of the model's overall \"attention.\"\n",
    "\n",
    "In the above, `Age` is the most important feature. \n",
    "\n",
    "\n",
    "Checking feature importance is useful because it helps you understand how your model is making predictions.\n",
    "\n",
    "- Feature importance reveals which inputs the model relies on most and provides a way to interpret the models predictions.\n",
    "- It can also identify unimportant features that can be removed to reducing noise and improve training time, model simplicity, and allow the model to better generalize to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0521c",
   "metadata": {},
   "source": [
    "\n",
    "## Improving Predictions\n",
    "\n",
    "Let's see if we can improve our predictions by adding a new feature to our training data.\n",
    "\n",
    "We might hypothesize that `Fare` is a _proxy_ for how likely a passenger is to be near a lifeboat given we expect cabins and rooms closer to the top deck to be more expensive than rooms in lower decks.\n",
    "\n",
    "Let's add `Fare` as a predictor and see the impact it has on the overall model accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144c77a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained with predictors: ['Age', 'Fare', 'Sex_male', 'Sex_female', 'Pclass_3', 'Pclass_1', 'Pclass_2']\n",
      "Feature importances:\n",
      " - Age: 0.288\n",
      " - Fare: 0.330\n",
      " - Sex_male: 0.147\n",
      " - Sex_female: 0.136\n",
      " - Pclass_3: 0.047\n",
      " - Pclass_1: 0.035\n",
      " - Pclass_2: 0.018\n",
      "Number of trees: 100\n",
      "Model accuracy: 84.36%\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_model(titanic_data, [\"Age\", \"Fare\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d83a92",
   "metadata": {},
   "source": [
    "\n",
    "Nice. Adding `Fare` improves our prediction accuracy by around another 3%.\n",
    "\n",
    "Interestingly, `Fare` turns out to have a greater feature importance that `Age`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b88685",
   "metadata": {},
   "source": [
    "\n",
    "## Dealing With Missing Data\n",
    "\n",
    "So far, we've ignored an important aspect of data engineering - we haven't been dealing with missing data.\n",
    "\n",
    "If you look at any of the advanced tutorials on Kaggle (such as the [Titanic - Advanced Feature Engineering Tutorial](https://www.kaggle.com/code/gunesevitan/titanic-advanced-feature-engineering-tutorial)) you will see that significant effort is spend identifying and handling missing data.\n",
    "\n",
    "Let's take a look at the data we've been using so far and identify missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5627f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ColumnName</th>\n",
       "      <th>MissingCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cabin</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Age</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Embarked</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ColumnName  MissingCount\n",
       "0      Cabin           687\n",
       "1        Age           177\n",
       "2   Embarked             2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def missing_counts(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns a DataFrame with the count of missing values in each column.\"\"\"\n",
    "    missing = (pd.DataFrame(df.isnull().sum(), columns=[\"MissingCount\"])\n",
    "                .sort_values(by=\"MissingCount\", ascending=False)\n",
    "                .reset_index()\n",
    "                .rename(columns={\"index\": \"ColumnName\"}))\n",
    "    return missing[missing[\"MissingCount\"] > 0]\n",
    "\n",
    "missing_counts(titanic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c526a7",
   "metadata": {},
   "source": [
    "\n",
    "Hmm. OK. We expect `Cabin` to have some significant missing values as not all passengers will have been able to afford or book a cabin. And we aren't currently using the `Embarked` column so we don't need to worry about the 2 rows with missing values in this column. But we are using `Age` and `Age` has a significant amount of missing data. \n",
    "\n",
    "Is this affecting the accuracy of our model? What can we do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f593f04",
   "metadata": {},
   "source": [
    "\n",
    "### Imputing Age\n",
    "\n",
    "Good data engineering practices states we should [deal with missing data](https://www.sixsideddice.com/Blog/DataEngineering/DealingWithMissingData.html) and one way of doing this is by `imputing` the values of missing data based on the distribution of values across the data set.\n",
    "\n",
    "So, how do we impute the `Age` column? \n",
    "\n",
    "There are several ways to fill in missing age values. A simple approach is to assign the mean or median age across the entire dataset. However, this often doesn't give the best results.\n",
    "\n",
    "A better strategy is to group the data by related attributes and compute the average age within each group. For example, passengers in the same class or with similar titles might have similar ages.\n",
    "\n",
    "To identify which features are related to age, we can use a **correlation matrix** to explore how `Age` is associated with other variables in the dataset.\n",
    "\n",
    "A **correlation matrix** is a table that shows the relationship between multiple variables in a dataset. Each cell in the matrix represents the correlation coefficient between two variables, which measures how strongly they move together.\n",
    "\n",
    " - A correlation close to +1 means the two variables increase or decrease together.\n",
    " - A correlation close to -1 means when one variable increases, the other decreases.\n",
    " - A correlation near 0 means there is little or no linear relationship.\n",
    "\n",
    "Correlation matrices help us quickly understand which variables are related and can guide decisions in data analysis, feature selection, and more.\n",
    "\n",
    "Let's create a correlation matrix to see which other features are strongly correlated with `Age`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c57412c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Age</td>\n",
       "      <td>Age</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Age</td>\n",
       "      <td>Pclass</td>\n",
       "      <td>0.369226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Age</td>\n",
       "      <td>SibSp</td>\n",
       "      <td>0.308247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Age</td>\n",
       "      <td>Parch</td>\n",
       "      <td>0.189119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Age</td>\n",
       "      <td>Fare</td>\n",
       "      <td>0.096067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Age</td>\n",
       "      <td>Survived</td>\n",
       "      <td>0.077221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Age</td>\n",
       "      <td>PassengerId</td>\n",
       "      <td>0.036847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature1     Feature2  Correlation\n",
       "24      Age          Age     1.000000\n",
       "23      Age       Pclass     0.369226\n",
       "25      Age        SibSp     0.308247\n",
       "26      Age        Parch     0.189119\n",
       "27      Age         Fare     0.096067\n",
       "22      Age     Survived     0.077221\n",
       "21      Age  PassengerId     0.036847"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_correlation_matrix(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Returns a DataFrame that is the correlation matrix of the DataFrame df.\"\"\"\n",
    "    return (df.select_dtypes(include='number')\n",
    "            .corr()\n",
    "            .abs()\n",
    "            .unstack()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"level_0\": \"Feature1\", \"level_1\": \"Feature2\", 0: \"Correlation\"}))\n",
    "\n",
    "matrix = create_correlation_matrix(titanic_data)\n",
    "matrix[matrix[\"Feature1\"] == \"Age\"].sort_values(by=\"Correlation\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f11ef7",
   "metadata": {},
   "source": [
    "\n",
    "The correlation matrix shows that `Age` is most strongly related to `Pclass`, so one option is to fill missing ages using the mean age within each Pclass. However, we also suspect that males and females in each class may have different average ages. To improve accuracy, we can group by both `Pclass` and `Sex`, and use the mean age within each group to impute missing values.\n",
    "\n",
    "So, first let's define a function to compute an imputation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c196f458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>34.611765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>41.281386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>28.722973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>30.740707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>21.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>26.507589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass     Sex        Age\n",
       "0       1  female  34.611765\n",
       "1       1    male  41.281386\n",
       "2       2  female  28.722973\n",
       "3       2    male  30.740707\n",
       "4       3  female  21.750000\n",
       "5       3    male  26.507589"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_mean_imputation_matrix(\n",
    "        df: pd.DataFrame, \n",
    "        for_col: str, \n",
    "        with_grouping: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Returns a DataFrame with the median values of forCol grouped by withGrouping.\"\"\"\n",
    "    return df.groupby(with_grouping)[for_col].mean().reset_index()\n",
    "\n",
    "age_impute_matrix = create_mean_imputation_matrix(titanic_data, \"Age\", [\"Pclass\", \"Sex\"])\n",
    "age_impute_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4ebe97",
   "metadata": {},
   "source": [
    "\n",
    "As suspected males and females in each class have different mean ages with males in 1st class tending to be oldest and females in 3rd class being youngest.\n",
    "\n",
    "We can now define a function to use this imputation matrix and apply the mean ages in these groups to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69d34652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_imputation_matrix(\n",
    "        df: pd.DataFrame, \n",
    "        imputation_matrix: pd.DataFrame, \n",
    "        for_col: str) -> pd.DataFrame:\n",
    "    \"\"\"Applies the imputation matrix to the DataFrame df.\"\"\"\n",
    "    grouping_columns = imputation_matrix.columns.values.tolist()\n",
    "    grouping_columns.remove(for_col) # type: ignore\n",
    "    df = df.copy()\n",
    "    for _, row in imputation_matrix.iterrows():\n",
    "        condition = (df[grouping_columns] == row[grouping_columns]).all(axis=1)\n",
    "        df.loc[condition & df[for_col].isnull(), for_col] = row[for_col]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab68cf",
   "metadata": {},
   "source": [
    "\n",
    "But before we apply the function, let's examine some of the passenger data where the passenger's age is unknown (we can then check the imputation has been applied correctly). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a685f0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PassengerId                           Name     Sex  Pclass  Age\n",
       "5             6               Moran, Mr. James    male       3  NaN\n",
       "17           18   Williams, Mr. Charles Eugene    male       2  NaN\n",
       "19           20        Masselmani, Mrs. Fatima  female       3  NaN\n",
       "26           27        Emir, Mr. Farred Chehab    male       3  NaN\n",
       "28           29  O'Dwyer, Miss. Ellen \"Nellie\"  female       3  NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_passengers = titanic_data[titanic_data[\"Age\"].isnull()]\n",
    "missing_passengers_ids = missing_passengers[\"PassengerId\"].tolist()\n",
    "missing_passengers[[\"PassengerId\", \"Name\", \"Sex\", \"Pclass\", \"Age\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3635a07b",
   "metadata": {},
   "source": [
    "\n",
    "Right, now let's create a copy of the data frame with imputed ages and verify that the missing data has been replaced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abab016c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ColumnName</th>\n",
       "      <th>MissingCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cabin</td>\n",
       "      <td>687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Embarked</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ColumnName  MissingCount\n",
       "0      Cabin           687\n",
       "1   Embarked             2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_data_with_imputed_age = apply_imputation_matrix(titanic_data, age_impute_matrix, \"Age\")\n",
    "missing_counts(titanic_data_with_imputed_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22a96d",
   "metadata": {},
   "source": [
    "\n",
    "Great. No more missing ages. Let's verify that the imputations are what we expect be looking at the updated passenger data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc5f18f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Moran, Mr. James</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>26.507589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Williams, Mr. Charles Eugene</td>\n",
       "      <td>male</td>\n",
       "      <td>2</td>\n",
       "      <td>30.740707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Masselmani, Mrs. Fatima</td>\n",
       "      <td>female</td>\n",
       "      <td>3</td>\n",
       "      <td>21.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Emir, Mr. Farred Chehab</td>\n",
       "      <td>male</td>\n",
       "      <td>3</td>\n",
       "      <td>26.507589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>O'Dwyer, Miss. Ellen \"Nellie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>3</td>\n",
       "      <td>21.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PassengerId                           Name     Sex  Pclass        Age\n",
       "5             6               Moran, Mr. James    male       3  26.507589\n",
       "17           18   Williams, Mr. Charles Eugene    male       2  30.740707\n",
       "19           20        Masselmani, Mrs. Fatima  female       3  21.750000\n",
       "26           27        Emir, Mr. Farred Chehab    male       3  26.507589\n",
       "28           29  O'Dwyer, Miss. Ellen \"Nellie\"  female       3  21.750000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(titanic_data_with_imputed_age[\n",
    "    titanic_data_with_imputed_age[\"PassengerId\"].isin(missing_passengers_ids)]\n",
    "    [[\"PassengerId\", \"Name\", \"Sex\", \"Pclass\", \"Age\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0405937",
   "metadata": {},
   "source": [
    "\n",
    "Perfect. Cross-referencing the imputation matrix above with the `Sex` and `Pclass` of each passenger we can see our imputation has been applied correct.\n",
    "\n",
    "New let's retrain the model on the new data and check the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80034b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained with predictors: ['Age', 'Fare', 'Sex_male', 'Sex_female', 'Pclass_3', 'Pclass_1', 'Pclass_2']\n",
      "Feature importances:\n",
      " - Age: 0.290\n",
      " - Fare: 0.328\n",
      " - Sex_male: 0.151\n",
      " - Sex_female: 0.132\n",
      " - Pclass_3: 0.049\n",
      " - Pclass_1: 0.033\n",
      " - Pclass_2: 0.017\n",
      "Number of trees: 100\n",
      "Model accuracy: 84.36%\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_model(titanic_data_with_imputed_age, [\"Age\", \"Fare\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b125323",
   "metadata": {},
   "source": [
    "\n",
    "Hmm. Hang on. This model has **exactly the same** model accuracy as the model without any fancy imputed data.\n",
    "\n",
    "_Why is this?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea16c49",
   "metadata": {},
   "source": [
    "\n",
    "### Surrogate Splits\n",
    "\n",
    "In version `1.3` SciKit Learn introduced support for [missing values in trees](https://github.com/scikit-learn/scikit-learn/pull/23595) via a technique know as `Surrogate Splits`. \n",
    "\n",
    "In fact, in versions prior to `1.3` (release July 2023), attempting to train a `DecisionTreeClassifier` or `RandomForestClassifier` on data that contain missing values would throw a `ValueError`. To train a model in earlier versions you would have to impute or remove any data with missing values.\n",
    "\n",
    "\n",
    "### How Surrogate Splits Work\n",
    "\n",
    "The [SciKit Learn documentation](https://scikit-learn.org/stable/modules/tree.html#tree-missing-value-support) provides some details how this works.\n",
    "\n",
    "In simple terms, during prediction, if all training data with missing values at a particular split ended up in the same class, then that class will be predicted for new any sample with missing values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2b8885a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- X <= 3.50\n",
      "|   |--- class: 0\n",
      "|--- X >  3.50\n",
      "|   |--- class: 1\n",
      "\n",
      "Prediction for NaN input: 1\n",
      "Expected 1 for NaN input, since this is the only class label with a NaN value in the data.\n"
     ]
    }
   ],
   "source": [
    "def explain_how_decision_trees_handle_missing_data(features : list[float], labels: list[int]):\n",
    "    X = np.array(features).reshape(-1, 1)\n",
    "    decision_tree = tree.DecisionTreeClassifier(random_state=0).fit(X, labels)\n",
    "\n",
    "    print(tree.export_text(decision_tree, feature_names=[\"X\"]))\n",
    "    prediction = decision_tree.predict(np.array([np.nan]).reshape(-1, 1))\n",
    "    print(f\"Prediction for NaN input: {prediction[0]}\")\n",
    "\n",
    "explain_how_decision_trees_handle_missing_data([0, 1, 6, np.nan], [0, 0, 1, 1])\n",
    "print(\"Expected 1 for NaN input, since this is the only class label with a NaN value in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922147d",
   "metadata": {},
   "source": [
    "\n",
    "If the split quality (criterion evaluation) is the same for both child nodes, the model breaks the tie for missing values during prediction by defaulting to the right node. During training, the splitter also considers a special case: placing all missing values in one child and all non-missing values in the other to determine if that produces a better split:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50e215a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- X <= 0.00\n",
      "|   |--- class: 0\n",
      "|--- X >  0.00\n",
      "|   |--- class: 1\n",
      "\n",
      "Prediction for NaN input: 1\n",
      "Expected 1 for NaN input, since the right node predicts a class label of 1.\n"
     ]
    }
   ],
   "source": [
    "explain_how_decision_trees_handle_missing_data([np.nan, -1, np.nan, 1], [0, 0, 1, 1])\n",
    "print(\"Expected 1 for NaN input, since the right node predicts a class label of 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079dd97",
   "metadata": {},
   "source": [
    "\n",
    "If no missing values are seen during training for a given feature, then during prediction missing values are mapped to the child with the most samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d950f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- X <= 1.50\n",
      "|   |--- class: 0\n",
      "|--- X >  1.50\n",
      "|   |--- class: 1\n",
      "\n",
      "Prediction for NaN input: 1\n",
      "Expected 1 for NaN input, since we have more 1 labels in the training data.\n"
     ]
    }
   ],
   "source": [
    "explain_how_decision_trees_handle_missing_data([1, 2, 3, 4], [0, 1, 1, 1])\n",
    "print(\"Expected 1 for NaN input, since we have more 1 labels in the training data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c4c95",
   "metadata": {},
   "source": [
    "\n",
    "## Final Thoughts\n",
    "\n",
    "Decision trees are powerful and interpretable tools, especially when you're getting started with classification. While this example was simple, it mirrors real-world machine learning workflows: data cleaning, feature selection, model training, and evaluation.\n",
    "\n",
    "Next Steps:\n",
    "\n",
    "- Try using more features like `Fare`, `Embarked`, or `SibSp`.\n",
    "- Experiment with different `max_depth` values.\n",
    "- Try other models like `RandomForestClassifier` for improved accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
